{
  "hash": "606d522de05ce0616f3f8e905c5f7d2b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2. Building analysis pipelines with `targets`\"\nfrom: markdown+emoji\ncode-annotations: hover\neval: false\nnumber-sections: true\n---\n\n\n\n\n[Skip to exercise \\>\\>](#exercise)\n\n# The `targets` package {#targets}\n\n## What does the `targets` package do?\n\nThe `{targets}` package is a framework for building data processing, analysis and visualization pipelines in R. It helps you structure your code by splitting it onto logical chunks that follow each other in a logical order. `{targets}` also manages the intermediate results of each step, so you do not have to do it manually. It also minimizes the time it takes to update the results if any step changes, be it some change in the source data, or in the code. `{targets}` knows which steps are up-to-date, and which are not, and only runs the steps that are outdated.\n\nConsider the following minimal example:\n\n```r\nlibrary(targets)\n\nlist(\n  # 1. Load the mtcars dataset.\n  tar_target(\n    name = mtcars_data,\n    command = data(mtcars)\n  ),\n\n  # 2. Compute the mean of the mpg column.\n  tar_target(\n    name = mpg_mean,\n    command = mean(mtcars_data$mpg)\n  ),\n\n  # 3. Create a histogram plot of mpg using base R.\n  tar_target(\n    name = hist_mpg,\n    packages = c(\"ggplot2\"),\n    command = {\n      ggplot(mtcars_data, aes(x = mpg)) +\n        geom_histogram(binwidth = 1) +\n        labs(title = \"Histogram of MPG\", x = \"Miles Per Gallon\", y = \"Count\")\n    }\n  )\n)\n```\n\nThis script has three steps. It loads the data, computes a simple statistic, and creates a plot. To run this pipeline with `targets` you would need to save this script to the `_targets.R` file in the root of the project directory and run `targets::tar_make()` in R console.\n\n<details>\n<summary>Code to try `targets` in R console interactively</summary>\n\nTo quickly run the pipeline above in R console without setting up `targets` properly and creating an R script file, you can use the code below, that can be pasted in to the R console (and it will make the pipeline run in a temporary directory):\n\n```r\nlibrary(targets)\ntar_dir({\n  tar_script({\n    library(targets)\n    list(\n      # Load the mtcars dataset.\n      tar_target(mtcars_data, mtcars),\n      \n      # Compute the mean of the mpg column.\n      tar_target(mpg_mean, mean(mtcars_data$mpg)),\n      \n      # Create a histogram plot of mpg using base R.\n      # The plot is saved as a PNG file and the filename is returned.\n      tar_target(plot_mpg, {\n        png(\"hist_mpg.png\")  # Open a PNG device.\n        hist(mtcars_data$mpg,\n             breaks = \"Sturges\",\n             main = \"Histogram of MPG\",\n             xlab = \"Miles Per Gallon\",\n             ylab = \"Frequency\",\n             col = \"lightblue\")\n        dev.off()  # Close the device.\n        \"hist_mpg.png\"  # Return the filename as the target value.\n      })\n    )\n  }, ask = FALSE)\n  \n  # Run all targets defined in the pipeline.\n  tar_make()\n\n})\n```\n\nYou will get output similar to this:\n\n```r\n▶ dispatched target mtcars_data\n● completed target mtcars_data [0 seconds, 1.225 kilobytes]\n▶ dispatched target plot_mpg\n● completed target plot_mpg [0.011 seconds, 65 bytes]\n▶ dispatched target mpg_mean\n● completed target mpg_mean [0 seconds, 52 bytes]\n▶ ended pipeline [0.059 seconds]\n```\n\nIt would mean that all steps have been executed successfully.\n\nSimilarly, if you add `tar_visnetwork()` after `tar_make()`, you will get a visualization of the pipeline:\n\n```r\nlibrary(targets)\ntar_dir({\n  tar_script({\n    library(targets)\n    list(\n      # Load the mtcars dataset.\n      tar_target(mtcars_data, mtcars),\n      \n      # Compute the mean of the mpg column.\n      tar_target(mpg_mean, mean(mtcars_data$mpg)),\n      \n      # Create a histogram plot of mpg using base R.\n      # The plot is saved as a PNG file and the filename is returned.\n      tar_target(plot_mpg, {\n        png(\"hist_mpg.png\")  # Open a PNG device.\n        hist(mtcars_data$mpg,\n             breaks = \"Sturges\",\n             main = \"Histogram of MPG\",\n             xlab = \"Miles Per Gallon\",\n             ylab = \"Frequency\",\n             col = \"lightblue\")\n        dev.off()  # Close the device.\n        \"hist_mpg.png\"  # Return the filename as the target value.\n      })\n    )\n  }, ask = FALSE)\n  \n  # Run all targets defined in the pipeline.\n  tar_make()\n  tar_visnetwork()\n})\n\n```\n\nYou would get output similar to the one in the @fig-mtcars-visnet below.\n</details>\n\n![\"Pipeline visualization example\"](media/images/mtcars-visnet.png){#fig-mtcars-visnet width=80%}\n\nAs you can see in @fig-mtcars-visnet, `{targets}` knows that the summary statistic and the plot depend on the data. If you were to change the first step of the pipeline, then the next time you ask `{targets}` to update the pipeline, it will only run steps two and three. If, however, instead of changing the data, you will change the code for the plot (e.g. change the title), then the pipeline will only re-run step three. This way, you do not have to manually keep track of what might have changed in the data or the code, or, in more complex pipelines, in the intermediate data (such as cross-validation folds for machine learning models) - `{targets}` will figure it out for you.\n\n# Exercise {#exercise}\n\n## Goal\n\nThe goal of this exercise is to use the project folder that you have set up in the previous exercise with `{renv}` to create a simple analysis pipeline using `{targets}`.\n\n::: {.callout-note}\nIf you have not successfully completed the previous exercise, you can start the current one by downloading the repository [https://github.com/e-kotov/2025-mpidr-workflows-reference-01](https://github.com/e-kotov/2025-mpidr-workflows-reference-01){target=\"_blank\"} and extracting it to your computer.\n:::\n\n## Instructions\n\n## Open the project folder in your editor\n\nOpen either your own project, or our reference project snapshot form the Note above.\n\nIf you open your own project, you should see in the R console:\n\n```r\n- Project 'path/on/your/computer/to/your/project' loaded. [renv 1.1.1]\n```\n\nIf you open our reference project, you should see in the R console:\n\n```r\nOK\n- Installing renv  ... OK\n\n- Project 'path/on/your/computer/2025-mpidr-workflows-reference-01' loaded. [renv 1.1.1]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\n```\n\n::: {.callout-note}\nThe reason you will see that the packages are not installed, is because the library of R packages is specific to an operating system and is considered as a disposable storage. By default, it is not pushed to GitHub. Therefore, when you copy such project from GitHub, you don't have the packages, but you have all the files to restore the packages as required by the `renv.lock` file.\n:::\n\n## Restore (install) the R packages in the project\n\n### In your own project\n\nIn your own project, you need to run:\n\n```r\nrenv::status()\n```\n\nThis is to make sure that all the packages are installed in the project. If any packages are still missing, see the step below.\n\n### In our reference project\n\nIf you are working in our reference project from [https://github.com/e-kotov/2025-mpidr-workflows-reference-01](https://github.com/e-kotov/2025-mpidr-workflows-reference-01){target=\"_blank\"}, you need to run:\n\n```r\nrenv::restore()\n```\n\nAfter you confirm, you should see the package installation process.\n\nOnce the installation finishes, to make sure you have successfully installed all packages, run:\n\n```r\nrenv::status()\n```\n\nYou should get something like this:\n\n```r\nNo issues found -- the project is in a consistent state.\n```\n\n## Setup the `{targets}` pipeline\n\nYou could of course create a `targets` pipeline manually, but there is a handy R function to initialize a template for you:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntargets::use_targets()\n```\n:::\n\n\n\n\nYou may get:\n\n```r\nℹ The package \"usethis\" is required.\n✖ Would you like to install it?\n\n1: Yes\n2: No\n```\n\nAgree to that and `{renv}` will take over again to install a new package. Don't forget to add it to the `renv.lock` file with `renv::snapshot()` later.\n\nOnce this is done, you should get a new file `_targets.R` in the root of your project folder.\n\n## Explore the sample pipeline\n\nBefore editing the `_targets.R` file, explore the pipeline that is already defined in the file.\n\nFirst, run this to see the interactive graph of the pipeline (you will need to agree to install `visNetwork` package when you run this for the first time, run the command again if you still don't see the visualisation in the viewer):\n\n```r\ntargets::tar_visnetwork()\n```\n\nYou will see that all steps are currently outdated.\n\nNow you can runn all the steps in the pipeline:\n\n```r\ntargets::tar_make()\n```\n\nNotice that you a new folder called `_targets` is created in the project folder. This is where the output of the pipeline will be stored as well as some metadata.\n\nIf you run `targets::tar_visnetwork()` again, you will find all steps are now up-to-date.\n\nNow you can inspect in the `R` console the results of each step. You do not need to think where each output is saved, as all outputs are always stored in the `_targets` folder and are accessible by their name. For example:\n\n```r\ntmp_object <- targets::tar_read(data)\nprint(tmp_object)\nrm(tmp_object)\n```\n\nThis will read the `data` object that is the output of the first step of the example pipeline and print it to the console. Finally, we also remove this object from the workspace to keep it clean.\n\nYou may also load the object `data` with its name directly into the workspace with:\n\n```r\ntargets::tar_load(data)\n```\n\n```r\nls()\n```\n\n```r\n[1] \"data\"\n```\n\nNow clean the workspace/environment:\n\n```r\nrm(list = ls())\n```\n\nTo delete saved outputs of all or certain (see the [documentation](https://docs.ropensci.org/targets/reference/tar_destroy.html){target=\"_blank\"}) steps, run:\n\n```r\ntargets::tar_destroy()\n```\n\nTo delete the saved results of steps that you might have deleted and will not use anymore, you could run:\n\n```r\ntargets::tar_prune()\n```\n\n## Replace the example pipeline with data analysis pipeline\n\nSee the code in Jonas's repository at [https://github.com/jschoeley/openscience25/tree/main/layer2-communal/example_2-1](https://github.com/jschoeley/openscience25/tree/main/layer2-communal/example_2-1){target=\"_blank\"}.\n\nLet us try to implement the steps from his code in a `targets` pipeline. We will guide you through the first pipeline step together.\n\n### Write code for data download step\n\nWe first need to implement the data download step found in [https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/10-download_input_data_from_zenodo.R](https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/10-download_input_data_from_zenodo.R){target=\"_blank\"}.\n\nWe will keep all functions that implement the steps in a separate folder called `R`. Create this folder manually and create a file called `10-download_input_data_from_zenodo.R` in it. Copy the code from Jonas's repository and save it in this new file, but convert it to a function that we could call from the `targets` pipeline. Compared to Jonas's original code, this function needs to make sure that the folder for the file downloads exists and also return the paths to the downloaded data files. See the suggested function below:\n\n```r\n# Download analysis input data from Zenodo\ndownload_files_from_zenodo <- function(\n  data_folder = \"data\"\n) {\n  # make sure the data folder exists\n  if (!dir.exists(data_folder)) {\n    dir.create(data_folder, recursive = TRUE)\n  }\n\n  # download the files\n  download.file(\n    url = 'https://zenodo.org/records/15033155/files/10-euro_education.csv?download=1',\n    destfile = paste0(data_folder, \"/10-euro_education.csv\")\n  )\n\n  download.file(\n    url = 'https://zenodo.org/records/15033155/files/10-euro_sectors.csv?download=1',\n    destfile = paste0(data_folder, \"/10-euro_sectors.csv\")\n  )\n\n  download.file(\n    url = 'https://zenodo.org/records/15033155/files/10-euro_geo_nuts2.rds?download=1',\n    destfile = paste0(data_folder, \"/10-euro_geo_nuts2.rds\")\n  )\n\n  # find the downloaded files\n  data_from_zenodo <- list.files(\"data\", full.names = TRUE)\n  names(data_from_zenodo) <- basename(data_from_zenodo)\n\n  # return the list of files\n  return(data_from_zenodo)\n}\n```\n\nCompare this to the original code from Jonas's repository at [https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/10-download_input_data_from_zenodo.R](https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/10-download_input_data_from_zenodo.R){target=\"_blank\"}.\n\nNow, to implement this first step into the pipeline, let us find the list of steps in the end of the `_targets.R` file, it's the one where all the steps are listed, each starting with `tar_target()`.\n\nRemove all the example steps and insert your own:\n\n```r\nlist(\n  # download files from zenodo\n  tar_target(\n    name = data_from_zenodo,\n    command = download_files_from_zenodo(\n      data_folder = \"data\"\n    ),\n    format = \"file\"\n  )\n)\n```\n\nNotice that we have added `format = \"file\"` to our first step. This means that `targets` should expect not a generic `R` object like a `data.frame`, `list`, or `vector` as on output of the `download_files_from_zenodo()` function, but a vector of files. This is useful for actions such as file downloads, as `targets` will be checking on it's own if all expected files are actually downloaded.\n\nThe final step is to inform `targets` that we want to run the pipeline by executing the `targets::tar_make()`.\n\n### Download files by executing the pipeline\n\nNow you can run the pipeline by executing:\n\n```r\ntargets::tar_make()\n```\n\nYou should get something like this:\n\n```r\n▶ dispatched target data_from_zenodo\ntrying URL 'https://zenodo.org/records/15033155/files/10-euro_education.csv?download=1'\nContent type 'text/plain; charset=utf-8' length 11150 bytes (10 KB)\n==================================================\ndownloaded 10 KB\n\ntrying URL 'https://zenodo.org/records/15033155/files/10-euro_sectors.csv?download=1'\nContent type 'text/plain; charset=utf-8' length 20626 bytes (20 KB)\n================================\ndownloaded 20 KB\n\ntrying URL 'https://zenodo.org/records/15033155/files/10-euro_geo_nuts2.rds?download=1'\nContent type 'application/octet-stream' length 261344 bytes (255 KB)\n=======\ndownloaded 255 KB\n\n● completed target data_from_zenodo [0.894 seconds, 293.12 kilobytes]\n▶ ended pipeline [0.94 seconds]\n```\n\nThe pipeline will download the files from Zenodo and will save them into the `data` folder. You can now use the downloaded files in the next steps of the pipeline.\n\n::: {.callout-note}\nOne thing to try, is to manually delete any one of the downloaded files and run the `targets::tar_visnetwork()` function. You will then see that one of the steps is outdated, as `targets` knows from the first run that it needs to keep track of files and it has noted the file signatures.\n:::\n\n### Add data folder to `.gitignore`\n\nAs the data is preserved at Zenodo, and as it is bad practice to store data in the git repository, we should add the `data` folder to the `.gitignore` file. You can do this by executing the following command in the `R` console:\n\n```r\nusethis::use_git_ignore(\"data\")\n```\n\nThis will create a new `.gitignore` file in the root of the project (in case it did not exist) and add the `data` folder there. Now `git` tools will not prompt you to commit the `data` folder and it will never be pushed to GitHub or other remote repository you are using.\n\n### `targets` settings\n\nFeel free to remove most of the comments in the example `_targets.R` file, espetially the ones regarding the targets options. We recommend you do set the options as follows:\n\n```r\ntar_option_set(\n  format = \"qs\"\n)\n```\n\nThe `qs` format is a faster and more space efficient alternative to `rds` format. You can read more about it in the [`qs2` documentation](https://github.com/qsbase/qs2?tab=readme-ov-file#single-threaded). This will be the format that `targets` uses to save the output of each step of the pipeline, unless you specify `format = 'file'`, like we did for the file list above, or set it to something else in the specicfic `tar_target()` step.\n\n::: {.callout-note}\nYou might also notice that we suggest to remove the `packages` option from `tar_option_set()` that was originally there. This is because it is much safer to specify the packages you need in each `tar_target()` step individually. This might seem like a lot of work, but it it has many advatanges:\n\n- Each step runs much faster, as unnecessary packages are not loaded.\n- You can prevent potential package conflicts. In big projects with hundreds of packages, some packages may not work well when loaded simultaneously (which would happen if you specify the `packages` option globally for all steps).\n- It is much easier to debug, as you can see which packages are loaded at each step.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
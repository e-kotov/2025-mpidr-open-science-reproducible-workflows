[
  {
    "objectID": "3b-build-docker-container-image.html",
    "href": "3b-build-docker-container-image.html",
    "title": "3b. Build your own Docker container image and run it (locally or in the academic cluster)",
    "section": "",
    "text": "First, make sure you got familiar with how Docker containers work.\nThis task can be done on your own computer or on a cloud-based Play with Docker environment. Specifically, for this tutorial we will use the Play with Docker Labs Environment, however you are free to follow along on your own computer, if you have Docker, OrbStack, or other Docker-compatible software installed. Using the Play with Docker Labs Environment is free and does not require any installation, only a web browser and a Docker Hub account.\n\nThe Goal\nBuild your own Docker container image for your hypothetical project. Imagine, that for this project you need 1 or 2 R packages that were archived from CRAN and are not available in the Rocker container image.\nTo achieve this goal, find a few archived R packages on CRAN. A few notable examples are rgdal, MortalitySmooth and many others. Remember, “40% of all packages ever in CRAN got at one point archived”[^1]. CRAN does not have it’s own section with archived packages, so you might want to look at CRANhaven Dashboard where you can find recently archived packages.\n\n\nChoose the Rocker Image version\nDepending on which archived package you selected, you must first find out which Rocker image version to use. For example, the package MortalitySmooth was archived on 2020-12-10. If you would use Rocker RStudio container image with R v4.1.0 released on 18th May 2021, the R package installer in the container will think it is 18th May 2021 and will try to install the package from the CRAN snapshot from that date. Since MortalitySmooth was archived after that date, you will not be able to install it using the standard install.packages() function. You will have to use remotes::install_version() function from the remotes package. Use internet search to find the release dates of R versions released just before the date the R package was archived. Rocker images are configured to use CRAN snapshot on the date of the R version release.\nSo your options are:\n\nUse more recent R version (and consequently Rocker image) and try to installing MortalitySmooth using remotes::install_version() function.\nUse strictly the R version that was released just before the package was archived and try to install the package using install.packages() function.\n\n\n\nStart the Play with Docker Environment\nGo to https://labs.play-with-docker.com/ and log in with your Docker Hub account.\n\n\n\n\n\nIf it is the first time you are logging in, Play With Docker will request access to your Docker account. Click on the “Accept” button to proceed. Than click the large green “Start” button to start the environment.\nClick on the “Add New Instance” button to start a new Docker container instance.\n\n\n\n\n\nYou should get a new terminal window with a prompt that looks like this:\n\n\n\n\n\n\n\nClone the Repository\nGo to the minimal example repository and copy the URL as shown below:\n\n\n\n\n\nNow clone the repository by pasting the following command into the terminal:\ngit clone https://github.com/Population-Dynamics-Lab/grid-sample-containerized.git\nCheck which folders you have in the current directory:\nls\nYou should see the grid-sample-containerized folder. Change the directory to the repository (you can type cd g and press Tab to autocomplete the folder name):\ncd grid-sample-containerized\nYou can check the contents of the repository by listing the files in the directory using in terminal again:\nls -al\n\n\nEdit the Dockerfile\nNow that you have selected the Rocker image version, you can edit the Dockerfile in the repository. The Dockerfile is a text file that contains instructions for building a Docker container image. Unlike in tutorial 1, In this tutorial, ignore the install.R file, we will be installing packages right in the Dockerfile.\nFind the editor button in the middle of the screen and click on it.\n\n\n\n\n\nA very simple file browser and editor (displayed when you click a file) will appear. You can edit the files in the repository directly in the browser. Remember to save changes.\n\n\n\n\n\nTo install the MortalitySmooth package in the Dockerfile add this in the second line:\nRUN install2.r --error --skipinstalled MortalitySmooth\nSo your final Dockerfile should look like this:\nFROM rocker/rstudio:4.0.0\nRUN install2.r --error --skipinstalled MortalitySmooth\nIf you were installing two packages, you would add them like this:\nFROM rocker/rstudio:4.0.0\nRUN install2.r --error --skipinstalled MortalitySmooth ggplot2\nRemember to save the changes.\n\n\nBuild the Docker Container Image\nUnlike in tutorial 1, you will not be using Binder web service to build the Docker container image automatically for you. Instead, you will use the docker command. The docker command is a command-line tool that allows you to interact with Docker containers and images. You can use the docker command to build a Docker image from the Dockerfile in the repository.\nFirst make sure you are in a folder that has the Dockerfile in it. You can check the contents of the current directory by running:\nls -al\nAnd you can quickly check what is in the Dockerfile by running:\ncat Dockerfile\nTo build the Docker image, run the following command in the terminal:\ndocker build -t r-mort-smooth:4.0.0 .\nLet us break down this command:\n\n\n\n\n\n\n\npart of command\nwhat it does\n\n\n\n\ndocker build\nThis is the base command used to build a Docker image from a Dockerfile.\n\n\n-t r-mort-smooth:4.0.0\nThe -t flag stands for “tag”. r-mort-smooth:4.0.0 is the name and tag given to the image. r-mort-smooth is the name of the image. 4.0.0 is the tag, which often represents the version of the image. You can choose any name and version here can also be anything, but we use the same version we used in the Dockerfile just so that we know which R version is going to be inside the container.\n\n\n.\nthis very important . (dot) specifies the build context, which is the current directory. Docker will look for a Dockerfile in this directory to create the image.\n\n\n\nThe container image will take about 3-6 minutes to build.\nWhen the build is finished, you can check that it was added to the local container image storage:\ndocker images\n\n\nRun the Docker Container from your Image\nNow you have a Docker container image with the MortalitySmooth package installed. To run it, you can use the docker run command in the following way:\ndocker run --rm -p 8787:8787 -v $(pwd):/home/rstudio/my-project -e PASSWORD=somepass r-mort-smooth:4.0.0\nLet us break down this command:\n\n\n\npart of command\nwhat it does\n\n\n\n\ndocker run\nThis is the base command used to run a Docker container from local or remote container image storage.\n\n\n--rm\nThis makes the container temporary. It will be destroyed after you stop it. You can explore other options (e.g. how to name containers, make them persistent and re-run the same ones after stopping) in the Docker documentation. But for now we want a disposable container that is destroyed after stopping.\n\n\n-p 8787:8787\nThis flag specifies that the port inside the container is mapped to your computer, so that you can access RStudio in a web browser. Briefly, RStudio in a container is actually a server software that works over a network and it is not exactly the same as RStudio on your laptop, even though it feels that way. This is why ports are necessary, but do not worry about it too much at the moment.\n\n\n-v $(pwd):/home/rstudio/my-project\nThis maps the current directory (designated by $(pwd)) from which you are running the command to a folder inside the container (/home/rstudio/my-project). Thanks to this, when you use the containerized RStudio, you will have access to your local folder and will be able to run scripts and edit them. The /home/rstudio/ is default for Rocker containers, and the my-project part can be replaced with anything. Instead of the current directory (designated by $(pwd)) you can provide /path/to/any/folder/on/your/computer.\n\n\n-e PASSWORD=somepass\nSets the password. Better use a good password, even though you are running locally.\n\n\nr-mort-smooth:4.0.0\nThe final part is the name and tag that you assigned earlier when you were creating the container image.\n\n\n\nThe container starts almost instantly. In case with Play with Docker service, you will see a button with a port pop-up:\n\n\n\n\n\nClick on the port number to open RStudio in a new tab. Use the default login rstudio and the password you set in the docker run command.\nIf you are following along on your own computer, you can open a web browser and go to http://localhost:8787 to access RStudio. Use the default login rstudio and the password you set in the docker run command.\nYou can now use RStudio in the browser to check if the MortalitySmooth package is installed by running:\nlibrary(MortalitySmooth)\n\n\nStop the container\nTo stop the container, click the “power” button in the top right corner of the RStudio window. Close the web browser tab with RStudio. In the Play with Docker browser tab, click in the terminal and press Ctrl+C or Ctrl+\\ to stop the container.\n\n\nVideo reference\nFor reference, here is the whole process in a sped up sequence:\n\n\n\n\n\nAnd her is a video in a more leisurely pace, where only the building of the Dockerfile is sped up, but you can watch and rewind to any steps:\n\n\n\nDiscussion\nNow that you have created your own reproducible repository, think for a moment, how future proof is it really? What does the reproducibility of your repository depend on? How can you further future-proof it?",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3b. Build your own `Docker` container image and run it (locally or in the academic cluster)"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "Creative Commons Legal Code\nCC0 1.0 Universal\nCREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\nLEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\nATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\nINFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\nREGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\nPROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\nTHE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED\nHEREUNDER.\nStatement of Purpose\nThe laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an “owner”) of an original work of authorship and/or a database (each, a “Work”).\nCertain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (“Commons”) that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.\nFor these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the “Affirmer”), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.\n\nCopyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (“Copyright and Related Rights”). Copyright and Related Rights include, but are not limited to, the following:\n\n\nthe right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work;\nmoral rights retained by the original author(s) and/or performer(s);\npublicity and privacy rights pertaining to a person’s image or likeness depicted in a Work;\nrights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below;\nrights protecting the extraction, dissemination, use and reuse of data in a Work;\ndatabase rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and\nother similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof.\n\n\nWaiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer’s Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “Waiver”). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer’s heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer’s express Statement of Purpose.\nPublic License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer’s express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer’s Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “License”). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer’s express Statement of Purpose.\nLimitations and Disclaimers.\n\n\nNo trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document.\nAffirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law.\nAffirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person’s Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.\nAffirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop Introduction",
    "section": "",
    "text": "This website provides supporting materials for the part 2 of the workshop Projects’ workflow for reproducibility and replicability using R held at the Second Rostock Open Science Workshop organized by the Max Planck Institute for Demographic Research in Rostock, Germany. This tutorial is licensed under CC0.\nThis tutorial includes the following sections:\nTo participate in the workshop, we recommend that you have a GitHub account and a Docker Hub account (the latter is optional and only needed for the last section on Docker).\nFor the section 3 on Docker, you may also want to install Docker on your computer (for Windows, macOS or Linux). For macOS we also highly recommend the free version of OrbStack as a complete and lightweight Docker Desktop replacement. In the workshop we will also show how to try Docker using just a web browser and the website https://labs.play-with-docker.com/ without installing any additional software on your computer.",
    "crumbs": [
      "Workshop Introduction"
    ]
  },
  {
    "objectID": "index.html#expected-learning-outcomes",
    "href": "index.html#expected-learning-outcomes",
    "title": "Workshop Introduction",
    "section": "Expected learning outcomes",
    "text": "Expected learning outcomes\n\nAble to explain why it is important to track the versions of software that are used for the analysis\nAble to use {renv} to setup a project directory with specific R package versions\nAble to explain the advantages of structuring the analysis code in a modular way\nAble to use {targets} to setup a modular analysis pipeline\nAble to explain the concept of containers and their role in computational reproducibility and discuss further reproducibility challenges\nAble to create a Docker container with RStudio and R of a specific version and install the R packages previously saved with {renv} into the container\n(optional) Able to create a GitHub repository that can be executed in the cloud using Binder",
    "crumbs": [
      "Workshop Introduction"
    ]
  },
  {
    "objectID": "3-containers.html",
    "href": "3-containers.html",
    "title": "3. Containerizing R and R Packages for Ultimate Reproducibility",
    "section": "",
    "text": "This section consists of three parts. Part a is a reference material, parts b and c are tutorials, but you probably will not have time to go through both in class, so choose one of them to do in class and the other one to do at home.\n\nThe overview of how containers work: 3a-how-docker-and-binder-work.qmd. This is more of a reference material that explains some technicalities of Docker (one of the popular containerization software) and Binder (an online service that allows you to run a container with R and RStudio in the cloud with just a web browser).\nA tutorial for building your own Docker container image with R and R packages pre-installed and run it locally. It covers the process of customizing the Dockerfile and building the container image. You do not have to have Docker installed on the computer as you can use the Play with Docker environment in the web browser. You would normally use such container image for everyday work locally on in the academic High Performance Computing (HPC) cluster.\nA tutorial for making your own git repository reproducible in Binder. It focuses on how to setup your git repository in such a way, so that anyone can run it in the cloud using Binder with just a web browser. The primary use case for this is publishing a reproducible repository for a research project, primarily with code that reproduces figures from raw data or pre-calculated and cached modelling results. Binder is limited in memory and compute power, so it will not handle long running resource intensive computation."
  },
  {
    "objectID": "2-analysis-pipeline.html",
    "href": "2-analysis-pipeline.html",
    "title": "2. Building analysis pipelines with targets",
    "section": "",
    "text": "Skip to exercise &gt;&gt;",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#what-does-the-targets-package-do",
    "href": "2-analysis-pipeline.html#what-does-the-targets-package-do",
    "title": "2. Building analysis pipelines with targets",
    "section": "1.1 What does the targets package do?",
    "text": "1.1 What does the targets package do?\nThe {targets} package is a framework for building data processing, analysis and visualization pipelines in R. It helps you structure your code by splitting it onto logical chunks that follow each other in a logical order. {targets} also manages the intermediate results of each step, so you do not have to do it manually. It also minimizes the time it takes to update the results if any step changes, be it some change in the source data, or in the code. {targets} knows which steps are up-to-date, and which are not, and only runs the steps that are outdated.\nConsider the following minimal example:\nlibrary(targets)\n\nlist(\n  # 1. Load the mtcars dataset.\n  tar_target(\n    name = mtcars_data,\n    command = data(mtcars)\n  ),\n\n  # 2. Compute the mean of the mpg column.\n  tar_target(\n    name = mpg_mean,\n    command = mean(mtcars_data$mpg)\n  ),\n\n  # 3. Create a histogram plot of mpg using base R.\n  tar_target(\n    name = hist_mpg,\n    packages = c(\"ggplot2\"),\n    command = {\n      ggplot(mtcars_data, aes(x = mpg)) +\n        geom_histogram(binwidth = 1) +\n        labs(title = \"Histogram of MPG\", x = \"Miles Per Gallon\", y = \"Count\")\n    }\n  )\n)\nThis script has three steps. It loads the data, computes a simple statistic, and creates a plot. To run this pipeline with targets you would need to save this script to the _targets.R file in the root of the project directory and run targets::tar_make() in R console.\n\n\nReveal code to try the toy targets example above in R console interactively\n\nTo quickly run the pipeline above in R console without setting up targets properly and creating an R script file, you can use the code below, that can be pasted in to the R console (and it will make the pipeline run in a temporary directory):\nlibrary(targets)\ntar_dir({\n  tar_script({\n    library(targets)\n    list(\n      # Load the mtcars dataset.\n      tar_target(mtcars_data, mtcars),\n      \n      # Compute the mean of the mpg column.\n      tar_target(mpg_mean, mean(mtcars_data$mpg)),\n      \n      # Create a histogram plot of mpg using base R.\n      # The plot is saved as a PNG file and the filename is returned.\n      tar_target(plot_mpg, {\n        png(\"hist_mpg.png\")  # Open a PNG device.\n        hist(mtcars_data$mpg,\n             breaks = \"Sturges\",\n             main = \"Histogram of MPG\",\n             xlab = \"Miles Per Gallon\",\n             ylab = \"Frequency\",\n             col = \"lightblue\")\n        dev.off()  # Close the device.\n        \"hist_mpg.png\"  # Return the filename as the target value.\n      })\n    )\n  }, ask = FALSE)\n  \n  # Run all targets defined in the pipeline.\n  tar_make()\n\n})\nYou will get output similar to this:\n▶ dispatched target mtcars_data\n● completed target mtcars_data [0 seconds, 1.225 kilobytes]\n▶ dispatched target plot_mpg\n● completed target plot_mpg [0.011 seconds, 65 bytes]\n▶ dispatched target mpg_mean\n● completed target mpg_mean [0 seconds, 52 bytes]\n▶ ended pipeline [0.059 seconds]\nIt would mean that all steps have been executed successfully.\nSimilarly, if you add tar_visnetwork() after tar_make(), you will get a visualization of the pipeline:\nlibrary(targets)\ntar_dir({\n  tar_script({\n    library(targets)\n    list(\n      # Load the mtcars dataset.\n      tar_target(mtcars_data, mtcars),\n      \n      # Compute the mean of the mpg column.\n      tar_target(mpg_mean, mean(mtcars_data$mpg)),\n      \n      # Create a histogram plot of mpg using base R.\n      # The plot is saved as a PNG file and the filename is returned.\n      tar_target(plot_mpg, {\n        png(\"hist_mpg.png\")  # Open a PNG device.\n        hist(mtcars_data$mpg,\n             breaks = \"Sturges\",\n             main = \"Histogram of MPG\",\n             xlab = \"Miles Per Gallon\",\n             ylab = \"Frequency\",\n             col = \"lightblue\")\n        dev.off()  # Close the device.\n        \"hist_mpg.png\"  # Return the filename as the target value.\n      })\n    )\n  }, ask = FALSE)\n  \n  # Run all targets defined in the pipeline.\n  tar_make()\n  tar_visnetwork()\n})\nYou would get output similar to the one in the Figure 1 below.\n\n\n\n\n\n\n\nFigure 1: “Pipeline visualization example”\n\n\n\nAs you can see in Figure 1, {targets} knows that the summary statistic and the plot depend on the data. If you were to change the first step of the pipeline, then the next time you ask {targets} to update the pipeline, it would only run steps two and three. If, however, instead of changing the data, you changed the code for the plot (e.g. change the title), then the pipeline would only re-run step three. This way, you do not have to manually keep track of what might have changed in the data or the code, or, in more complex pipelines, in the intermediate data (such as cross-validation folds for machine learning models) - {targets} will figure it out for you.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#goal",
    "href": "2-analysis-pipeline.html#goal",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.1 Goal",
    "text": "2.1 Goal\nThe goal of this exercise is to use the project folder that you have set up in the previous exercise with {renv} to create a simple analysis pipeline using {targets}.\n\n\n\n\n\n\nNote\n\n\n\nIf you have not successfully completed the previous exercise, you can start the current one by downloading the repository https://github.com/e-kotov/2025-mpidr-workflows-reference-01 and extracting it to your computer.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#instructions",
    "href": "2-analysis-pipeline.html#instructions",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.2 Instructions",
    "text": "2.2 Instructions",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#open-the-project-folder-in-your-editor",
    "href": "2-analysis-pipeline.html#open-the-project-folder-in-your-editor",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.3 Open the project folder in your editor",
    "text": "2.3 Open the project folder in your editor\nOpen either your own project, or our reference project snapshot form the Note above.\nIf you open your own project, you should see in the R console:\n- Project 'path/on/your/computer/to/your/project' loaded. [renv 1.1.1]\nIf you open our reference project, you should see in the R console:\nOK\n- Installing renv  ... OK\n\n- Project 'path/on/your/computer/2025-mpidr-workflows-reference-01' loaded. [renv 1.1.1]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\n\n\n\n\n\n\nNote\n\n\n\nThe reason you will see that the packages are not installed, is because the library of R packages is specific to an operating system and is considered as a disposable storage. By default, it is not pushed to GitHub. Therefore, when you copy such project from GitHub, you don’t have the packages, but you have all the files to restore the packages as required by the renv.lock file.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#restore-install-the-r-packages-in-the-project",
    "href": "2-analysis-pipeline.html#restore-install-the-r-packages-in-the-project",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.4 Restore (install) the R packages in the project",
    "text": "2.4 Restore (install) the R packages in the project\n\n2.4.1 In your own project\nIn your own project, you need to run:\nrenv::status()\nThis is to make sure that all the packages are installed in the project. If any packages are still missing, see the step below.\n\n\n2.4.2 In our reference project\nIf you are working in our reference project from https://github.com/e-kotov/2025-mpidr-workflows-reference-01, you need to run:\nrenv::restore()\nAfter you confirm, you should see the package installation process.\nOnce the installation finishes, to make sure you have successfully installed all packages, run:\nrenv::status()\nYou should get something like this:\nNo issues found -- the project is in a consistent state.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#setup-the-targets-pipeline",
    "href": "2-analysis-pipeline.html#setup-the-targets-pipeline",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.5 Setup the {targets} pipeline",
    "text": "2.5 Setup the {targets} pipeline\nYou could of course create a targets pipeline manually, but there is a handy R function to initialize a template for you:\n\ntargets::use_targets()\n\nAgree to that and {renv} will take over again to install a new package. Don’t forget to add it to the renv.lock file with renv::snapshot() later.\nOnce this is done, you should get a new file _targets.R in the root of your project folder.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#explore-the-sample-pipeline",
    "href": "2-analysis-pipeline.html#explore-the-sample-pipeline",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.6 Explore the sample pipeline",
    "text": "2.6 Explore the sample pipeline\nBefore editing the _targets.R file, explore the pipeline that is already defined in the file.\nFirst, run this to see the interactive graph of the pipeline (you will need to agree to install visNetwork package when you run this for the first time, run the command again if you still don’t see the visualisation in the viewer):\ntargets::tar_visnetwork()\nYou will see that all steps are currently outdated.\nNow you can runn all the steps in the pipeline:\ntargets::tar_make()\nNotice that you a new folder called _targets is created in the project folder. This is where the output of the pipeline will be stored as well as some metadata.\nIf you run targets::tar_visnetwork() again, you will find all steps are now up-to-date.\nNow you can inspect in the R console the results of each step. You do not need to think where each output is saved, as all outputs are always stored in the _targets folder and are accessible by their name. For example:\ntmp_object &lt;- targets::tar_read(data)\nprint(tmp_object)\nrm(tmp_object)\nThis will read the data object that is the output of the first step of the example pipeline and print it to the console. Finally, we also remove this object from the workspace to keep it clean.\nYou may also load the object data with its name directly into the workspace with:\ntargets::tar_load(data)\nls()\n[1] \"data\"\nNow clean the workspace/environment:\nrm(list = ls())\nTo delete saved outputs of all or certain (see the documentation) steps, run:\ntargets::tar_destroy()\nTo delete the saved results of steps that you might have deleted and will not use anymore, you could run:\ntargets::tar_prune()",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#adjust-targets-settings",
    "href": "2-analysis-pipeline.html#adjust-targets-settings",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.7 Adjust targets settings",
    "text": "2.7 Adjust targets settings\nFeel free to remove most of the comments in the example _targets.R file, espetially the ones regarding the targets options. We recommend you do set the options as follows:\ntar_option_set(\n  format = \"qs\"\n)\nThe qs/qs2 format is a faster and more space efficient alternative to rds format. You can read more about it in the qs2 documentation. This will be the format that targets uses to save the output of each step of the pipeline, unless you specify format = 'file', or set it to something else in the specicfic tar_target() step.\n\n\n\n\n\n\nNote\n\n\n\nYou might also notice that we suggest to remove the packages option from tar_option_set() that was originally there. This is because it is much safer to specify the packages you need in each tar_target() step individually. This might seem like a lot of work, but it it has many advatanges:\n\nEach step runs much faster, as unnecessary packages are not loaded.\nYou can prevent potential package conflicts. In big projects with hundreds of packages, some packages may not work well when loaded simultaneously (which would happen if you specify the packages option globally for all steps).\nIt is much easier to debug, as you can see which packages are loaded at each step.\n\n\n\nFor more targets options, you can see the documentation for tar_option_set().",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#step1-get-data",
    "href": "2-analysis-pipeline.html#step1-get-data",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.8 Analysis step 1 - Get the data",
    "text": "2.8 Analysis step 1 - Get the data\nSee the code in Jonas’s repository at https://github.com/jschoeley/openscience25/tree/main/layer2-communal/example_2-1.\nLet us try to implement the steps from his code in a targets pipeline. We will guide you through the first pipeline step together.\n\n2.8.1 Wrap data download step into a function\nWe first need to implement the data download step found in https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/10-download_input_data_from_zenodo.R.\nWe will keep all functions that implement the steps in a separate folder called R. Create this folder manually and create a file called 10-download_input_data_from_zenodo.R in it. Copy the code from Jonas’s repository and save it in this new file, but convert it to a function that we could call from the targets pipeline. Compared to Jonas’s original code, this function needs to make sure that the folder for the file downloads exists and also return the paths to the downloaded data files. See the suggested function below:\n# Download analysis input data from Zenodo\ndownload_files_from_zenodo &lt;- function(\n  data_folder = \"data\"\n) {\n  # make sure the data folder exists\n  if (!dir.exists(data_folder)) {\n    dir.create(data_folder, recursive = TRUE)\n  }\n\n  # download the files\n  download.file(\n    url = 'https://zenodo.org/records/15033155/files/10-euro_education.csv?download=1',\n    destfile = paste0(data_folder, \"/10-euro_education.csv\")\n  )\n\n  download.file(\n    url = 'https://zenodo.org/records/15033155/files/10-euro_sectors.csv?download=1',\n    destfile = paste0(data_folder, \"/10-euro_sectors.csv\")\n  )\n\n  download.file(\n    url = 'https://zenodo.org/records/15033155/files/10-euro_geo_nuts2.rds?download=1',\n    destfile = paste0(data_folder, \"/10-euro_geo_nuts2.rds\")\n  )\n\n  # find the downloaded files\n  data_from_zenodo &lt;- list.files(\"data\", full.names = TRUE)\n\n  # return the list of files\n  return(data_from_zenodo)\n}\nCompare this to the original code from Jonas’s repository at https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/10-download_input_data_from_zenodo.R.\nNow, to implement this first step into the pipeline, let us find the list of steps in the end of the _targets.R file, it’s the one where all the steps are listed, each starting with tar_target().\nRemove all the example steps and insert your own:\nlist(\n  # download files from zenodo\n  tar_target(\n    name = data_from_zenodo,\n    command = download_files_from_zenodo(\n      data_folder = \"data\"\n    ),\n    format = \"file\"\n  )\n)\nNotice that we have added format = \"file\" to our first step. This means that targets should expect not a generic R object like a data.frame, list, or vector as on output of the download_files_from_zenodo() function, but a vector of files. This is useful for actions such as file downloads, as targets will be checking on it’s own if all expected files are actually downloaded.\nThe final step is to inform targets that we want to run the pipeline by executing the targets::tar_make().\n\n\n2.8.2 Download files by executing the pipeline\nNow you can run the pipeline by executing:\ntargets::tar_make()\nYou should get something like this:\n▶ dispatched target data_from_zenodo\ntrying URL 'https://zenodo.org/records/15033155/files/10-euro_education.csv?download=1'\nContent type 'text/plain; charset=utf-8' length 11150 bytes (10 KB)\n==================================================\ndownloaded 10 KB\n\ntrying URL 'https://zenodo.org/records/15033155/files/10-euro_sectors.csv?download=1'\nContent type 'text/plain; charset=utf-8' length 20626 bytes (20 KB)\n================================\ndownloaded 20 KB\n\ntrying URL 'https://zenodo.org/records/15033155/files/10-euro_geo_nuts2.rds?download=1'\nContent type 'application/octet-stream' length 261344 bytes (255 KB)\n=======\ndownloaded 255 KB\n\n● completed target data_from_zenodo [0.894 seconds, 293.12 kilobytes]\n▶ ended pipeline [0.94 seconds]\nThe pipeline downloaded the files from Zenodo and will save them into the data folder. You can now use the downloaded files in the next steps of the pipeline.\nMake an interactive graph of the pipeline:\ntargets::tar_visnetwork()\nYou should see a network similar to the one in Figure 2.\n\n\n\n\n\n\nFigure 2: “Up to date workflow including getting the data and producing the basemap”\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing to try, is to manually delete any one of the downloaded files and run the targets::tar_visnetwork() function. You will then see that one of the steps is outdated, as targets knows from the first run that it needs to keep track of files and it has noted the file signatures.\n\n\n\n\n2.8.3 Add data folder to .gitignore\nAs the data is preserved at Zenodo, and as it is bad practice to store data in the git repository, we should add the data folder to the .gitignore file. You can do this by executing the following command in the R console:\nusethis::use_git_ignore(\"data\")\nThis will create a new .gitignore file in the root of the project (in case it did not exist) and add the data folder there. Now git tools will not prompt you to commit the data folder and it will never be pushed to GitHub or other remote repository you are using.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#analysis-step-2---create-background-map",
    "href": "2-analysis-pipeline.html#analysis-step-2---create-background-map",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.9 Analysis step 2 - Create background map",
    "text": "2.9 Analysis step 2 - Create background map\nNow let us add a new step that creates background map that will be re-used in all of the plots me can create following the code in Jonas’s repository. We will look at the code in https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/20-create_european_backgroundmap.R and adjust it to be a targets step executed by tar_target().\nBecause the code there uses _config.yaml to customize the plots, you will need to download it and put into your project, e.g. into config folder.\nWe will also break down the code into steps, as it makes sense to isolate the code for downloading the data from Natural Earth into a separate function.\n\n2.9.0.1 Step 2.1 Download Natural Earth data\nLook at the corresponding lines in Jonas’s code:\n# Download Eurasian geodata ---------------------------------------\neura_sf &lt;-\n  # download geospatial data for European, Asian and African countries\n  ne_countries(continent = c('europe', 'asia', 'africa'),\n               returnclass = 'sf', scale = 10) %&gt;%\n  # project to crs suitable for Europe\n  st_transform(crs = config$crs) %&gt;%\n  # merge into single polygon\n  st_union(by_feature = FALSE) %&gt;%\n  # crop to Europe\n  st_crop(xmin = config$eurocrop$xmin,\n          xmax = config$eurocrop$xmax,\n          ymin = config$eurocrop$ymin,\n          ymax = config$eurocrop$ymax)\nWe will rewrite it as a following function:\n# Download Eurasian geodata ---------------------------------------\ndownload_ne_data &lt;- function(\n  crs, xmin, xmax, ymin, ymax\n) {\n  eura_sf &lt;-\n    # download geospatial data for European, Asian and African countries\n    rnaturalearth::ne_countries(\n      continent = c('europe', 'asia', 'africa'),\n      returnclass = 'sf',\n      scale = 10\n    ) |&gt;\n    # project to crs suitable for Europe\n    sf::st_transform(crs = crs) |&gt;\n    # merge into single polygon\n    sf::st_union(by_feature = FALSE) |&gt;\n    # crop to Europe\n    sf::st_crop(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax)\n\n  return(eura_sf)\n}\nAs you can see, we got rid of the references to yaml file, which is one of the ways to approach it. Any preferences previously set in yaml file are now set in the function parameters.\nWe also prefixed each function call with a package name. We do this to be more aware of which packages we use in each function to minimize the potential for conflicts.\nIn the list of targets steps we will add the call as follows:\ntar_target(\n  name = eura_sf,\n  packages = c(\"rnaturalearth\", \"sf\"),\n  command = download_ne_data(\n    crs = 3035,\n    xmin = 25.0e+5,\n    xmax = 75.0e+5,\n    ymin = 13.5e+5,\n    ymax = 54.5e+5\n  )\n)\nNote that:\n\nWe specified the packages argument for an individual target step.\nWe did not specify the format = \"file\", so by default it will be qs2 format, as specified above in tar_option_set().\n\n\n\n\n\n\n\nNote\n\n\n\nAs you might have noticed, we set the name of the targets step as eura_sf, and we also return an object called eura_sf from the download_ne_data() function. These names do NOT have to match, as in larger projects functions may be reused multiple times and they may have more generic uses and therefore return different output to different targets based on their input parameters. In this case, we are just naming the objects in the same way for convenience.\n\n\nTest the targets pipeline by running it and making an interactive graph:\ntargets::tar_make()\ntargets::tar_visnetwork()\nYou should see a network similar to the one in Figure 3.\n\n\n\n\n\n\nFigure 3: “Up to date workflow including getting the data and producing the basemap”",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#step-2.2-wrap-background-map-creation-step-into-a-function",
    "href": "2-analysis-pipeline.html#step-2.2-wrap-background-map-creation-step-into-a-function",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.10 Step 2.2 Wrap background map creation step into a function",
    "text": "2.10 Step 2.2 Wrap background map creation step into a function\nCreate a file 20-create_european_backgroundmap.R in R subfolder of your project.\nLook at the first few lines in Jonas’s code, just after the library() calls (which we will look at later).\nOriginal lines:\n# input and output paths\npaths &lt;- list()\npaths$input &lt;- list(\n  config = './code/_config.yaml'\n  \n)\npaths$output &lt;- list(\n  euro_basemap.rds = './data/20-euro_basemap.rds'\n)\n\n# global configuration\nconfig &lt;- read_yaml(paths$input$config)\nWe do not nede to use any of these lines anymore. Previously we had to specify where to save the base map in paths$output. With targets, we do not need to manually keep track where we save intermediate results and wether or not they are up to date. Instead of saving the file at the path specified in paths$output, we will just return an R object from the function called by a particular targets step and it will be stored automatically for us in _targets/objects folder. We do not need the _config.yaml here anymore, as it did not contain any variables that would affect the basemap creation.\nWe only need the lines that create a ggplot2 plot object with the basemap:\n# Draw a basemap of Europe ----------------------------------------\neuro_basemap &lt;-\n  ggplot(eura_sf) +\n  geom_sf(\n    aes(geometry = geometry),\n    color = NA, fill = 'grey90'\n  ) +\n  coord_sf(expand = FALSE, datum = NA) +\n  theme_void()\nAnd we rewrite these as a function:\n# Draw a basemap of Europe ----------------------------------------\ncreate_european_backgroundmap &lt;- function(\n  eura_sf\n) {\n  # eura_sf &lt;- targets::tar_read(eura_sf) # for debug\n\n  euro_basemap &lt;-\n    ggplot2::ggplot(eura_sf) +\n    ggplot2::geom_sf(\n      aes(geometry = geometry),\n      color = NA,\n      fill = 'grey90'\n    ) +\n    ggplot2::coord_sf(expand = FALSE, datum = NA) +\n    ggplot2::theme_void()\n\n  return(euro_basemap)\n}\nNotice that we pass eura_sf object that we created with targets step that executed download_ne_data() and we return the ggplot2 object euro_basemap that should be reused in the next steps for creating the maps.\nWe also added explicit ggplot2:: prefixes to relevant functions. This is optional, as we will specify that we need ggplot2 package for this function in the targets pipeline, but this also helps to debug the function quickly.\nTo run the code in this function manually, you can use the commented out line eura_sf &lt;- targets::tar_read(eura_sf) to quickly load the eura_sf object (assuming you have previously ran the targets pipeline and it is saved in the targets/ojects storage).\nNow let us add a targets step to the pipeline:\ntar_target(\n  name = euro_basemap,\n  packages = c(\"ggplot2\"),\n  command = create_european_backgroundmap(\n    eura_sf = eura_sf\n  )\n)\nTest the targets pipeline by running it and making an interactive graph:\ntargets::tar_make()\ntargets::tar_visnetwork()\nYou should see a network similar to the one in Figure 4.\n\n\n\n\n\n\nFigure 4: “Up to date workflow including getting the data and producing the basemap”\n\n\n\nNow if you run targets::tar_read(euro_basemap), you should get a plot of the basemap (see Figure 5):\n\n\n\n\n\n\nFigure 5: The figure to be expected as output of targets::tar_read(euro_basemap)",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "2-analysis-pipeline.html#next-steps",
    "href": "2-analysis-pipeline.html#next-steps",
    "title": "2. Building analysis pipelines with targets",
    "section": "2.11 Next steps",
    "text": "2.11 Next steps\nNow you it is up to you to adapt the code from one of the R scripts that create maps (30-plot_figure_1.R, 31-plot_figure_2.R, 32-plot_figure_3.R) in Jonas’s repository at https://github.com/jschoeley/openscience25/tree/main/layer2-communal/example_2-1/code. in the same way we did above. We would suggest that because these plots are final output images, instead of returning ggplot2 objects from the plot making functions, you save the plots to files and return the path to the file, like we did with the step that downloaded the data from Zenodo.",
    "crumbs": [
      "2. Building analysis pipelines with `targets`"
    ]
  },
  {
    "objectID": "3c-reproducible-github-repo-in-binder.html",
    "href": "3c-reproducible-github-repo-in-binder.html",
    "title": "3c. Make your own git repository reproducible in Binder",
    "section": "",
    "text": "First, make sure you got familiar with how Docker containers work.",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3c. Make your own git repository reproducible in `Binder`"
    ]
  },
  {
    "objectID": "3c-reproducible-github-repo-in-binder.html#footnotes",
    "href": "3c-reproducible-github-repo-in-binder.html#footnotes",
    "title": "3c. Make your own git repository reproducible in Binder",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCRANhaven: Study: Many Archived Packages Return to CRAN↩︎",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3c. Make your own git repository reproducible in `Binder`"
    ]
  },
  {
    "objectID": "1-r-packages.html",
    "href": "1-r-packages.html",
    "title": "1. R packages version management with renv",
    "section": "",
    "text": "Skip to exercise &gt;&gt;",
    "crumbs": [
      "1. R packages version management with `renv`"
    ]
  },
  {
    "objectID": "1-r-packages.html#renv",
    "href": "1-r-packages.html#renv",
    "title": "1. R packages version management with renv",
    "section": "1.1 {renv}",
    "text": "1.1 {renv}\n\n1.1.1 What does {renv} do?\n{renv} takes over the location of the R package library and the command install.packages(). After initializing {renv} for a particular project/working directory, all your package installations and removals will be handled by {renv}. From now on, whenever you use install.packages() in that project, {renv} will install the package into the project directory.\n\n\n\n\n\n\nNote\n\n\n\nActually, {renv} will install the packages into user cache folder defined by the operating system (this is different from the default user folder for R package installation), and will “link” the packages into your project (or projects) directory(-ies). For example, you have two projects with different versions of {ggplot2}, but the same versions of {dplyr}. The {renv} packages cache will have two versions of {ggplot2} and one version of {dplyr}. Your projects however, will only contain “links” to the cache, therefore, you will not have two identical copies of {dplyr} on disk in two project directories, as they will both just point to the cached version of {dplyr} in {renv}’s cache folder. You can read more about it here\n\n\n\n\n1.1.2 Under the hood of renv\nWhen you install {renv} and run renv::init() in the current project/working directory, it creates several files and directories in your current project/working directory (see Figure 1).\n\n\n\n\n\n\nFigure 1: Files and folders created by renv when initialized\n\n\n\n\n{renv} creates an invisible .Rprofile text file your current project/working directory. This file may contain any R code, and when R/RStudio is started in that directory, the commands in this file get executed. {renv} adds a line source(\"renv/activate.R\") which runs a script that activates {renv} for this particular project every time R/RStudio is started in this directory.\n{renv} also creates an renv folder, where it will store all R scripts that it requires to function, and also the packages that you install for this particular project.\n{renv} also creates an renv.lock file, which stores the exact versions of R packages you install, as well as the sources where you install them from. That is, it keeps track if the package was installed from CRAN, Bioconductor, GitHub repository, or any other source.",
    "crumbs": [
      "1. R packages version management with `renv`"
    ]
  },
  {
    "objectID": "1-r-packages.html#pak",
    "href": "1-r-packages.html#pak",
    "title": "1. R packages version management with renv",
    "section": "1.2 {pak}",
    "text": "1.2 {pak}\n\n1.2.1 What does {pak} do?\n{pak} can be used instead of R’s default install.packages() to install R packages and their dependencies faster.\nIt can also serve as a so-called “backend” for {renv}, that is it will help {renv} to install packages and their dependencies much faster when you, or someone who reproduces your code, such as your collaborator, use {renv} to fully restore the R package environment of a project on a new computer.\n\n\n\n\n\n\nWarning\n\n\n\nIf you would like to enable {pak} as a backend for {renv}, you can find which {renv} options to set in the {renv} documentation, however, as this is still experimental, please do not use it in this tutorial, as we will not have time to debug it if something does not work.",
    "crumbs": [
      "1. R packages version management with `renv`"
    ]
  },
  {
    "objectID": "1-r-packages.html#goal",
    "href": "1-r-packages.html#goal",
    "title": "1. R packages version management with renv",
    "section": "2.1 Goal",
    "text": "2.1 Goal\nThe goal of this exercise is setup a new blank project with isolated R packages library using {renv} in such a way, so that all the specific package versions can be restored on a new computer or in a new folder.\nThe tutorial is long and detailed, however in essence it is just a few steps. In Figure 2 you can find a summary of the steps and can click on them to get directly to the corresponding section of the exercise.\n\n\n\n\n\n\nflowchart TD\n    A[\"Create project folder & set package repository\"] --&gt; B[\"Initialize renv: **renv::init()**\"]\n    B --&gt; C[\"Configure renv: set snapshot type to 'all' and enable Posit Package Manager\"]\n    C --&gt; D[\"Install packages (e.g., targets)\"]\n    D --&gt; E[\"Snapshot package versions: **renv::snapshot()**\"]\n    E --&gt; F[\"Test restoration: copy project, delete renv/library, run **renv::restore()**\"]\n\n    click A \"#create-project\"\n    click B \"#renv-init\"\n    click C \"#renv-config\"\n    click D \"#install-packages\"\n    click E \"#snapshot\"\n    click F \"#test-restore\"\n\n\n\n\nFigure 2: Tutorial steps overview\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can find the repository with the expected end result of the exercise at https://github.com/e-kotov/2025-mpidr-workflows-reference-01. If you would like to skip the exercise and just see how the package restoration process with {renv} works, feel free to download/clone this repository and follow the instructions starting with the Test R packages restoration step.",
    "crumbs": [
      "1. R packages version management with `renv`"
    ]
  },
  {
    "objectID": "1-r-packages.html#instructions",
    "href": "1-r-packages.html#instructions",
    "title": "1. R packages version management with renv",
    "section": "2.2 Instructions",
    "text": "2.2 Instructions\n\n2.2.1 Create a new project folder\nCreate a new folder wherever you would normally create a folder for a new research project.\nIf you are using RStudio, use the menu File -&gt; New Project, then select New Directory -&gt; New Project and set the folder name and location. You may select Use renv with this project in the end of the project creation wizard, but we would advice to skip it for now, as we will set this up in the next step.\nIf you are using Visual Studio Code or the new Positron (the future replacement for RStudio based on Visual Studio Code), just create a folder manually and open it in the respective editor.\nIf you are using some other editor, follow the usual procedure you know to create a new project.\nIn any case, once you are ready, run the following command in the R console to make sure that the current working directory is the project directory you intended to create and use:\n\ngetwd()\n\n\n\n2.2.2 Setup packages repository\nBy default, your R installation probably installs packages from CRAN ( https://cran.r-project.org/ ) or from CRAN mirror hosted by RStudio/Posit ( https://cran.rstudio.com/ ). These repositories are updated every day with latest R package versions, so you never know, which version of a package you will get when you install it. You can check which repository is used with:\n\ngetOption(\"repos\")\n\nYou will probably see:\nCRAN \n\"https://cran.rstudio.com\" \nor:\n CRAN\n\"@CRAN@\"\nTo have more control over which versions are installed, you may set the repository to Posit Package Manager and constrain it to a certain date. We also add the rOpenSci repository to install a certain package that is only available from there.\n\noptions(\n  repos = c(\n    \"CRAN\" = \"https://packagemanager.posit.co/cran/2025-02-28\"\n  )\n)\n\nIf you run getOption(\"repos\") again, you should see the URLs you have entered above. If it is set, the package versions you install will always correspond to the date you used. This allows you to freeze the packages you use for your project in time and prevent installation of new versions which may break your code. You may change the date for this setting later and update the packages, test your code with newer versions and decide if you need to revert to older versions.\n\n\n\n\n\n\nNote\n\n\n\nUnlike CRAN, Posit Package Manager provides pre-compiled binaries of R packages for Linux. This is particularly beneficial for Linux users and for last steps in this tutorial, where we build containers that are also based on Linux. Typically, R packages on Linux are installed from source, which can be time-consuming, and installing pre-compiled binaries speeds up the installation process significantly.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe repos option should be set before you move to the next step and initialise {renv}, as at the time of initialisation, {renv} will use the current value of the repos option. The option may be changed later, including manually in the renv.lock file.\n\n\n\n\n2.2.3 Install {renv}\nInstall {renv} as you normally would. You should not install any other packages at this point, as after installing and initizalising {renv}, you would need to install then again, as R will start using your project directory as a package library and will not see any previously user-installed packages anymore.\n\ninstall.packages(\"renv\")\n\n\n\n2.2.4 Initialise {renv}\nInitialise {renv} in the current project/working directory:\n\nrenv::init()\n\nYou should get a message like this (your R and {renv} versions may be different):\nThe following package(s) will be updated in the lockfile:\n\n# CRAN -----------------------------------------------------------------------\n- renv   [* -&gt; 1.1.1]\n\nThe version of R recorded in the lockfile will be updated:\n- R      [* -&gt; 4.4.3]\n\n- Lockfile written to \"path-to-your-project/renv.lock\".\n- renv activated -- please restart the R session.\n\n\n\n\n\n\nImportant\n\n\n\nAt this point, you need to restart the R session for {renv} to work. Do not ignore this instruction, as otherwise the rest of the steps will not work.\n\n\nFeel free to double check if the {renv} related files have been created in your project, as was shown in Figure 1 in the previous section.\n\n\n2.2.5 Customize the {renv} configuration\nBefore you proceed, a few options related to {renv} operation should be set manually.\n\n2.2.5.1 Force {renv} to record all packages\nBy default, {renv} tries to automatically identify which packages you actually use in your project and adds those to the lockfile renv.lock which would later be used to restore the package versions on a new computer. It does this by scanning all *.R script files in your project/working directory and tries to find lines like library(ggplot2). Unfortunately, it does not always do it well, and you may also have another way of loading the packages that {renv} does not recognize.\nTherefore, it is better to just force {renv} to keep track not just the packages it “thinks” you use, but all packages you install while working with the current project. To do that, run the folliwing code in the R console:\n\nrenv::settings$snapshot.type(value = \"all\")\n\nThis will change the setting in the renv/settings.json file. You could of course also open the file in editor and change the setting manually. You can find more information on this setting in the official {renv} documentation.\n\n\n2.2.5.2 Instruct {renv} to use Posit Package Manager\nWe have already set the repos option above to the Posit Package Manager, but it is also worth enabling the corresponding option in {renv} with:\n\nrenv::settings$ppm.enabled(value = TRUE)\n\n\n\n\n2.2.6 Try installing packages\nNow you can try to install a new package to see if {renv} works as expected. In the next exercise, we will need the {targets} and {usethis} packages, so we can try installing them:\n\ninstall.packages(c(\"targets\", \"usethis\"))\n\nYou should see in the R console:\nThe following package(s) will be installed:\n- backports  [1.5.0]\n...\n- targets    [1.10.1]\n...\n- yaml       [2.3.10]\nThese packages will be installed into \"path-to-your-project/renv/library/macos/R-4.4/aarch64-apple-darwin20\".\n\nDo you want to proceed? [Y/n]: \nAgree and wait for the packages and their dependencies to be installed.\nYou can check that the {targets} package is installed by trying to load it:\n\nlibrary(targets)\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have not had targets installed prior to starting this exercise, it does not exist in your default user R library, but only exists in the {renv} R package cache and in the current project directory. If you would like to check this, you can start another R/RStudio session in a differnt project directory and try to load targets there. It should fail, as there is no targets package in the default user R library.\n\n\nNow go to https://github.com/jschoeley/openscience25/tree/main/layer2-communal/example_2-1/code, find the relevant R script, and check all the packages that are needed to run Jonas’s code and install them into the project just like you have just done with targets.\n\n\n\n\n\n\nNote\n\n\n\nNotice that some packages are installed from CRAN, but there is one package that is installed from https://ropensci.r-universe.dev. This is not a problem at all for {renv}, as it is able to memorise where each package is coming from, even if it is from multiple repositories.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou basically need all lines from https://github.com/jschoeley/openscience25/blob/main/layer2-communal/example_2-1/code/00-install_dependencies.R except for the last three where Jonas saves the package list to a csv file. You do not need it, as we are using {renv} to manage packages.\n\n\n\n\n\n2.2.7 Save the package versions into lockfile\nYou can install and remove packages as usual while working with your project. Whenever you are done with testing your code and are certain that the installed packages are suffient to reproduce your code, you can “lock” the package list and their versions with {renv}.\nTo check the current status of the R package versions snapshot, you can run the following command:\n\nrenv::status()\n\nYou should get something like this:\nThe following package(s) are in an inconsistent state:\n\npackage     installed recorded used\naskpass     y         n        y   \nbackports   y         n        y   \n....\nyaml        y         n        y   \nzip         y         n        y   \n\nSee `?renv::status` for advice on resolving these issues.\nAs you can see from the recorded column, none of the package versions are currently recorded (snapshotted) in the renv.lock lockfile. To save the package list and their versions into the lockfile, you can run the following command:\n\nrenv::snapshot()\n\nYou may get a warning message for some missing dependencies, which you may have to install manually. For example:\nThe following required packages are not installed:\n- cpp11  [required by igraph]\n- progress       [required by readxl, vroom]\n- RcppArmadillo  [required by bayesm]\nConsider reinstalling these packages before snapshotting the lockfile.\nIf you get something like this, you should install the missing package(s) and then run renv::snapshot() again.\n\ninstall.packages(c(\"cpp11\", \"progress\", \"RcppArmadillo\", \"qs2\"))\nrenv::snapshot()\n\nThen check the status again:\n\nrenv::status()\n\nYou should repeat this until after running renv::status() you get:\nNo issues found -- the project is in a consistent state.\n\n\n\n\n\n\nNote\n\n\n\nYou can also manually open the renv.lock file in the editor and search for pacakges such as targets to see if they were snapshotted successfully.\n\n\n\n\n2.2.8 Test R packages restoration\nIn the steps above you have created a project directory with fully isolated independent R package library. Now is the time to do a simple test, whether you can restore the R package environment of a project on a new computer, or at least in a new folder.\nTo quickly test with the new folder, close your R/RStudio (or whichever other editor you are using) and create a of the the project folder in a new location on your computer. Once copied, manually delete the renv/library folder at this new location. Now open the new project folder with the deleted renv/library folder in your editor.\n\n\n\n\n\n\nNote\n\n\n\nInstead of using your own project, you can also use the https://github.com/e-kotov/2025-mpidr-workflows-reference-01 repository for the test. Just download/clone it to your computer and proceed with the steps below.\n\n\nOnce R starts in this new project directory you should see the following message:\nOK\n- Installing renv  ... OK\n\n- Project 'path/to/your/project-copy' loaded. [renv 1.1.1]\n- One or more packages recorded in the lockfile are not installed.\n- Use `renv::status()` for more details.\nThis means that R found the .Rrofile file in the project root and exectuted all of the commands there. The first command was to run all the code in renv/activate.R which would automatically install {renv} on a new computer if it was not installed yet. It has also checked briefly the installation status of the packages recorded in the renv.lock file against the project package library, ignoring any packages installed by the user into the system wide user R packages library.\nFeel free to run renv::status() to check the status of the packages, but this is optional.\nTo restore all pacakge versions, run renv::restore(prompt = FALSE). Once the process finishes, you should be able to check the status again:\n\nrenv::status()\n\nIn the end you should be able to get No issues found -- the project is in a consistent state. when running renv::status().\n\n\n\n\n\n\nNote\n\n\n\nYou can also manually check if the renv/library folder has been created in this new copy of the original project and if it contains subfolders for the packages.",
    "crumbs": [
      "1. R packages version management with `renv`"
    ]
  },
  {
    "objectID": "3a-how-docker-and-binder-work.html",
    "href": "3a-how-docker-and-binder-work.html",
    "title": "3a. Get Familiar with Docker and Binder",
    "section": "",
    "text": "The goal of this tutorial is to show you how to create a reproducible GitHub repository that can be run in Binder. This is a great way to share your code with others, as they can run your code in the cloud using just their web browser without having to install anything on their local machine. There are some limitations, such operating memory limit of 1-2 GB and temporary nature of such environment, but for most small to medium-sized projects and for someone to just quickly inspect your code, this should be sufficient.",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3a. Get Familiar with `Docker` and `Binder`"
    ]
  },
  {
    "objectID": "3a-how-docker-and-binder-work.html#real-world-examples",
    "href": "3a-how-docker-and-binder-work.html#real-world-examples",
    "title": "3a. Get Familiar with Docker and Binder",
    "section": "Real world examples",
    "text": "Real world examples\nBelow you can find real-world examples of such repositories that accompany scientific papers:\n\nhttps://github.com/Gchism94/NestArchOrg\nhttps://github.com/bbartholdy/mb11CalculusPilot\nhttps://github.com/parkgayoung/racisminarchy\n\nThere is also a minimal example (Kotov and Deneke 2024) that you can see in action below. Mind you, that the video is sped up, and in reality, it may takes a few minutes to start the container.\n\n\n\nPerforming gridsample technique in a cloud cointainer in Binder",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3a. Get Familiar with `Docker` and `Binder`"
    ]
  },
  {
    "objectID": "3a-how-docker-and-binder-work.html#try-the-example-repository-live-demo",
    "href": "3a-how-docker-and-binder-work.html#try-the-example-repository-live-demo",
    "title": "3a. Get Familiar with Docker and Binder",
    "section": "Try the example repository live-demo",
    "text": "Try the example repository live-demo\nGo to this example repository. Try to run the contents of the repository in a container in the cloud using Binder by clicking on the badge that looks like this: .\nWhen you do so, you will be presented with an RStudio interface in your web browser. The actual RStudio and R will be running in a container on a server provided by one of the participants of the The BinderHub Federation.\nHow does this magic happen?\n\n\n\n\n\nflowchart TB\n  A[User passes GitHub repository URL to mybinder.org]\n  A --&gt; Z[BinderHub starts a cloud server with Docker software]\n  Z --&gt; B[BinderHub downloads files from given repository to create a Docker image]\n  B --&gt; C{Docker image already in temp storage?}\n  C -- Yes --&gt; D[BinderHub pulls Docker image from temp storage]\n  C -- No --&gt; E[BinderHub Builds Docker image]\n  E --&gt; F[BinderHub pushes Docker image to temp storage]\n  F --&gt; D\n  D --&gt; G[BinderHub runs the Docker image]\n  G --&gt; H[User gets access to the running image in a web browser]",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3a. Get Familiar with `Docker` and `Binder`"
    ]
  },
  {
    "objectID": "3a-how-docker-and-binder-work.html#explore-how-containers-work",
    "href": "3a-how-docker-and-binder-work.html#explore-how-containers-work",
    "title": "3a. Get Familiar with Docker and Binder",
    "section": "Explore how containers work",
    "text": "Explore how containers work\nIf you look inside the example repository, you will see that it contains a Dockerfile and a few other essential files to create and run a container with RStudio. Here’s a breakdown of what each file does and how you can change them to set up your container.\n\nDockerfile\nDockerfile defines how your container is created. It is an instruction set that container building software, such as Docker, uses to create a container image. It is similar to how you can type commands in the terminal to install software on your computer, but in this case, you are installing software inside a container and the syntax of a Dockerfile is a bit different, as it prefixes each command with a keyword such as FROM, COPY, RUN, etc. You can find more details in the Docker documentation.\n\nFirst we tell the container software to use the rocker/binder:4.0.0 container image as a base. This is a container image that someone has already created and shared with the community. In this case it’s created by the Rocker Project. It already has Linux operating system, R, RStudio and many R packages installed.\n\nFROM rocker/binder:4.0.0\nIf you were building your own highly specialised container, you would probably start with a more basic container image, such as ubuntu:20.04 or debian:bullseye, and then install all the software you need from scratch. This is more work, but it gives you more control over what is installed in the container.\n\nNext we copy all the files from the repository into the container. This is done with the COPY command. This way, when the container is started, all the files from the repository are available inside the container.\n\nCOPY --chown=${NB_USER} . ${HOME}\nIf you were to build a container not for the GitHub repository and Binder, but for use in the high-performance computing cluster or on your own computer, you would probably want to copy only the essential files and not the whole repository. You would also not include the data or the analysis code, but would only include configuration files that define which software should be inside the container.\n\nThe last line of the Dockerfile is the RUN command that installs R packages that are not part of the Rocker container image. You can find out empirically which packages are missing by trying to run your code in the container and seeing which packages are missing. Or you can read the source code of the container creation script (e.g. for Rocker Verse, Rocker Tidyverse, or Rocker Geospatial) and see which packages are pre-installed there.\n\nRUN if [ -f install.R ]; then R --quiet -f install.R; fi\n\n\ninstall.R\ninstall.R file contains the R code that installs the GridSample package. This package is not included in the Rocker container image, so we need to install it manually. In this case, a very specific version of the package is installed, because this specific version had a certain bug fixed.\nremotes::install_github(\"nrukt00vt/gridsample@03c2d10134cbf94dc8c7452c3a5967c8624e260a\", force = TRUE, dependencies = TRUE)\nHowever, this install.R file can contain any R code. You can install packages in all the usual ways, for example:\nTo install from CRAN:\ninstall.packages(\"ggplot2\")\nTo install a certain package version from CRAN or install versions of packages that were archived on CRAN (by default, installs the latest available version, even if the package is archived):\nremotes:install_version(\"MortalitySmooth\")\nTo install any package from GitHub:\nremotes::install_github(\"MPIDR/rsocsim\")",
    "crumbs": [
      "3. Containerizing R and R Packages for Ultimate Reproducibility",
      "3a. Get Familiar with `Docker` and `Binder`"
    ]
  }
]